{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, values):\n",
    "        self.values = values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X, Y = self.values[index]\n",
    "        return np.array(X).astype(\"float\"), np.array(Y).astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(path):\n",
    "    lines = list()\n",
    "    with open(path) as file:\n",
    "        csv_reader = file.readlines()\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            row = re.split(r'\\t+', row.rstrip('\\t'))[1:]\n",
    "            values = row.pop(-1)\n",
    "            values = values.split(';')\n",
    "            for r in values:\n",
    "                chunk = re.findall('\\d*\\.?\\d+',r)\n",
    "                chunk = list(map(float, chunk)) \n",
    "                row.append(chunk)\n",
    "            lines.append(row)\n",
    "    features = []\n",
    "    for x in lines:\n",
    "        features.append([x[2], x[3], x[4], max(x[10])])\n",
    "    filterUnk = list(filter(lambda x: 'Unknown' not in x, features))\n",
    "    notGauges = ['PowerPICC', 'PICC', 'PowerPort', 'CentralLine', 'Other']\n",
    "    notivLoc = ['Other', 'PowerPort', 'CentralLine', 'PICC', 'PowerPICC']\n",
    "    filterGauge = list(filter(lambda x: x[0] not in notGauges, filterUnk))\n",
    "    filterIV = list(filter(lambda x: x[1] not in notivLoc, filterGauge))\n",
    "    return filterIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = readFile(\"SN-SRMC-CT-1.txt\") + readFile(\"SN-SRMC-CT-2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Gauge Types:',\n",
       " 6,\n",
       " 'IV Locations:',\n",
       " 10,\n",
       " 'Protocols:',\n",
       " 3360,\n",
       " 'Pressures:',\n",
       " 25891)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaugeTypes = set()\n",
    "ivLocation = set()\n",
    "protocol = dict()\n",
    "pressure = set()\n",
    "for inputX in original:\n",
    "    gaugeTypes.add(inputX[0])\n",
    "    ivLocation.add(inputX[1])\n",
    "    if inputX[2] in protocol:\n",
    "        protocol[inputX[2]] = protocol[inputX[2]] + [inputX[3]]\n",
    "    else:\n",
    "        protocol[inputX[2]] = [inputX[3]]\n",
    "    pressure.add(inputX[3])\n",
    "\n",
    "\"Gauge Types:\", len(gaugeTypes), \"IV Locations:\", len(ivLocation), \"Protocols:\", len(protocol), \"Pressures:\", len(pressure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseProtocol(x):\n",
    "    protocol = x.split(\" \")\n",
    "    Contrast = 0\n",
    "    Saline = 0\n",
    "    Mixed = 0\n",
    "    AmountC = 0\n",
    "    AmountS = 0\n",
    "    AmountM = 0\n",
    "    PercentM = 0\n",
    "    Flowrate = 0\n",
    "    for i in protocol:\n",
    "        if i[0] == \"@\":\n",
    "            FlowRate = float(i[1:])\n",
    "        if i[0] == \"C\":\n",
    "            Contrast = 1\n",
    "            AmountC = int(i[1:])\n",
    "        if i[0] == \"S\":\n",
    "            Saline = 1\n",
    "            AmountS = int(i[1:])\n",
    "        if i[0] == \"R\":\n",
    "            rprot = i.split(\"%\")\n",
    "            Mixed = 1\n",
    "            AmountM = int(rprot[0][1:])\n",
    "            PercentM = int(rprot[1][1:])\n",
    "    return [Contrast, Saline, Mixed, AmountC, AmountS, AmountM, PercentM, FlowRate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundup(x):\n",
    "    return int(math.ceil(x/100.0))*100\n",
    "\n",
    "mapGauge = dict()\n",
    "for i, x in enumerate(sorted(list(gaugeTypes))):\n",
    "    mapGauge[x] = i\n",
    "mapIV = dict()\n",
    "for i, x in enumerate(sorted(list(ivLocation))):\n",
    "    mapIV[x] = i\n",
    "mapPressure = dict()\n",
    "buckets = []\n",
    "count = 0\n",
    "bucketSize = 20\n",
    "for p in range(int(min(pressure)), int(roundup(max(pressure))), bucketSize):\n",
    "    buckets.append(p)\n",
    "    mapPressure[p] = count\n",
    "    count += 1\n",
    "result = []\n",
    "for v in original:\n",
    "    result.append(([mapGauge[v[0]], mapIV[v[1]]] + parseProtocol(v[2]) , float(v[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.524817525434855 34.73501216666802\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "for i in range(len(result)-1, 0, -1): \n",
    "      \n",
    "    # Pick a random index from 0 to i  \n",
    "    j = random.randint(0, i + 1)  \n",
    "    \n",
    "    # Swap arr[i] with the element at random index  \n",
    "    result[i], result[j] = result[j], result[i]      \n",
    "\n",
    "half = int(len(result)//2)\n",
    "\n",
    "training_set = result[:half]\n",
    "validation_set = result[half:]\n",
    "\n",
    "validation_p = np.array([p[1] for p in validation_set])\n",
    "\n",
    "mean = np.mean(validation_p)\n",
    "std = np.std(validation_p)\n",
    "\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8 if cuda else 0 \n",
    "\n",
    "train_dataset = MyDataset(training_set)\n",
    "train_loader_args = dict(shuffle=True, batch_size=64, num_workers=num_workers, pin_memory=True) if cuda                    else dict(shuffle=True, batch_size=64)\n",
    "train_loader = DataLoader(train_dataset, **train_loader_args)\n",
    "\n",
    "val_dataset = MyDataset(validation_set)\n",
    "val_loader_args = dict(shuffle=False, batch_size=64, num_workers=num_workers, pin_memory=True) if cuda                    else dict(shuffle=False, batch_size=1)\n",
    "val_loader = DataLoader(val_dataset, **val_loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = len(train_dataset.__getitem__(0)[0])\n",
    "output_size = 1\n",
    "hidden_size = int(input_size**2)//2\n",
    "\n",
    "input_size, output_size, hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.act1 = nn.Sigmoid()\n",
    "        self.b1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "        self.b2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.act3 = nn.Sigmoid()\n",
    "        self.b3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.layer4 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_val):\n",
    "        h = input_val\n",
    "        h = self.layer1(h)\n",
    "        h = self.act1(h)\n",
    "        h = self.b1(h)\n",
    "        h = self.layer2(h)\n",
    "        h = self.act2(h)\n",
    "        h = self.b2(h)\n",
    "        h = self.layer3(h)\n",
    "        h = self.act3(h)\n",
    "        h = self.b3(h)\n",
    "        h = self.layer4(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNetwork(\n",
      "  (layer1): Linear(in_features=10, out_features=32, bias=True)\n",
      "  (act1): Sigmoid()\n",
      "  (b1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (act2): Sigmoid()\n",
      "  (b2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (act3): Sigmoid()\n",
      "  (b3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer4): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MyNetwork(input_size, 32, output_size)\n",
    "model = model.float()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(x, y):\n",
    "    total = 0\n",
    "    for v in range(len(x)):\n",
    "        if x[v] and y[v]:\n",
    "            total += 1\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_predictions = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    correct_predictions_0_5 = 0.0\n",
    "    correct_predictions_1 = 0.0\n",
    "    correct_predictions_2 = 0.0\n",
    "    correct_predictions_5 = 0.0\n",
    "    correct_predictions_10 = 0.0\n",
    "    correct_predictions_20 = 0.0\n",
    "    correct_predictions_g_20 = 0.0\n",
    "        \n",
    "    start_time = time.time()\n",
    "    og_time = start_time\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "        data = data.to(device)\n",
    "        target = target.to(device) # all data & model on same device\n",
    "\n",
    "        outputs = model(data.float()).double().squeeze(1)\n",
    "        \n",
    "        total_predictions += target.size(0)\n",
    "        correct_predictions += torch.sum(torch.abs(outputs - target) < 2).item()\n",
    "        correct_predictions_0_5 += torch.sum(torch.abs(outputs - target) <= 0.5).item()\n",
    "        correct_predictions_1 += combine(torch.abs(outputs - target) > 0.5,  torch.abs(outputs - target) <= 1)\n",
    "        correct_predictions_2 += combine(torch.abs(outputs - target) > 1,  torch.abs(outputs - target) <= 2)\n",
    "        correct_predictions_5 += combine(torch.abs(outputs - target) > 2,  torch.abs(outputs - target) <= 5)\n",
    "        correct_predictions_10 += combine(torch.abs(outputs - target) > 5,  torch.abs(outputs - target) <= 10)\n",
    "        correct_predictions_20 += combine(torch.abs(outputs - target) > 10,  torch.abs(outputs - target) <= 20)\n",
    "        correct_predictions_g_20 += torch.sum(torch.abs(outputs - target) > 20).item()\n",
    "\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    running_loss /= len(train_loader)\n",
    "    acc = (correct_predictions/total_predictions)*100.0\n",
    "    print('Training Accuracy: ', acc, \"    \", 'Training Loss: ', running_loss)\n",
    "    return running_loss, acc, [correct_predictions_0_5, correct_predictions_1, correct_predictions_2, correct_predictions_5, correct_predictions_10, correct_predictions_20, correct_predictions_g_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(model, dev_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        correct_predictions_0_5 = 0.0\n",
    "        correct_predictions_1 = 0.0\n",
    "        correct_predictions_2 = 0.0\n",
    "        correct_predictions_5 = 0.0\n",
    "        correct_predictions_10 = 0.0\n",
    "        correct_predictions_20 = 0.0\n",
    "        correct_predictions_g_20 = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(dev_loader):   \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(data.float()).squeeze(1)\n",
    "\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += torch.sum(torch.abs(outputs - target) < 2).item()\n",
    "            correct_predictions_0_5 += torch.sum(torch.abs(outputs - target) <= 0.5).item()\n",
    "            correct_predictions_1 += combine(torch.abs(outputs - target) > 0.5,  torch.abs(outputs - target) <= 1)\n",
    "            correct_predictions_2 += combine(torch.abs(outputs - target) > 1,  torch.abs(outputs - target) <= 2)\n",
    "            correct_predictions_5 += combine(torch.abs(outputs - target) > 2,  torch.abs(outputs - target) <= 5)\n",
    "            correct_predictions_10 += combine(torch.abs(outputs - target) > 5,  torch.abs(outputs - target) <= 10)\n",
    "            correct_predictions_20 += combine(torch.abs(outputs - target) > 10,  torch.abs(outputs - target) <= 20)\n",
    "            correct_predictions_g_20 += torch.sum(torch.abs(outputs - target) > 20).item()\n",
    "                \n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        running_loss /= len(dev_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Validation Accuracy: ', acc, \"    \", 'Validation Loss: ', running_loss)\n",
    "        return running_loss, acc, [correct_predictions_0_5, correct_predictions_1, correct_predictions_2, correct_predictions_5, correct_predictions_10, correct_predictions_20, correct_predictions_g_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training Accuracy:  0.020885547201336674      Training Loss:  29.571586322757703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type MyNetwork. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "\r",
      "  3%|▎         | 1/30 [00:13<06:37, 13.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.008950681743592804      Validation Loss:  29.366443915637692\n",
      "Saving the Model Version 0\n",
      "===================================================================================================================\n",
      "Epoch 1\n",
      "Training Accuracy:  0.17603532641126626      Training Loss:  29.00397296006766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 2/30 [00:27<06:22, 13.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.40576423904287373      Validation Loss:  28.48428324783408\n",
      "Saving the Model Version 1\n",
      "===================================================================================================================\n",
      "Epoch 2\n",
      "Training Accuracy:  0.7399451008473565      Training Loss:  28.051723595917085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 3/30 [00:40<06:07, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.6504162067010771      Validation Loss:  27.45105122512059\n",
      "Saving the Model Version 2\n",
      "===================================================================================================================\n",
      "Epoch 3\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_epochs = 30\n",
    "Train_loss = []\n",
    "Train_acc = []\n",
    "Val_loss = []\n",
    "Val_acc = []\n",
    "val_a_acc = []\n",
    "train_a_acc = []\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter(\"./runs/training\")\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(\"Epoch \" + str(epoch)) \n",
    "            \n",
    "    train_l, train_a, t_all_a = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_l, val_a, v_all_a = val_model(model, val_loader, criterion)\n",
    "    \n",
    "    scheduler.step(train_l)\n",
    "    \n",
    "#     for name, param in model.named_parameters():\n",
    "#         writer.add_histogram('grad_' + name, param.grad.data, epoch)\n",
    "    \n",
    "    Train_loss.append(train_l)\n",
    "    Train_acc.append(train_a)\n",
    "    Val_loss.append(val_l)\n",
    "    Val_acc.append(val_a)\n",
    "    val_a_acc.append(v_all_a)\n",
    "    train_a_acc.append(v_all_a)\n",
    "    \n",
    "#     writer.add_scalars('loss', {'train': train_l, 'val': val_l}, epoch)\n",
    "#     writer.add_scalars('acc', {'train': train_a, 'val': val_a}, epoch)\n",
    "    \n",
    "    print(\"Saving the Model Version \" + str(epoch))\n",
    "    torch.save(model, './model_final_r.pt')\n",
    "    torch.save(optimizer, './adam_model_final_r.pt')\n",
    "    print('='*115)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = ('[0, 0.5]', '(0.5, 1]', '(1, 2]', '(2, 5]', '(5, 10]', '(10, 20]', '(20, Inf.)')\n",
    "performanceV = val_a_acc[-1]\n",
    "performanceT = train_a_acc[-1]\n",
    "\n",
    "x = np.arange(len(objects))\n",
    "# print(v_all_a)\n",
    "width = 0.2 # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, performanceV, width, label=\"Validation\")\n",
    "rects1 = ax.bar(x + width/2, performanceT, width, label=\"Training\")\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Training Points')\n",
    "ax.set_title('Accuracy for Last Epoch')\n",
    "ax.set_xticks(x)\n",
    "labels = []\n",
    "totalV = sum(performanceV)\n",
    "totalT = sum(performanceT)\n",
    "\n",
    "for x in range(len(objects)):\n",
    "    labels.append(objects[x] + \"\\n\" + str(int((performanceV[x]/totalV)*100)) + \"%,\" + str(int((performanceT[x]/totalT)*100)) + \"%\")\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "plt.savefig('Acc_Last_Epoch.png', dpi=300)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "objects = ('[0, 0.5]', '(0.5, 1]', '(1, 2]', '(2, 5]', '(5, 10]', '(10, 20]', '(20, Inf.)')\n",
    "performanceV = val_a_acc[0]\n",
    "performanceT = train_a_acc[0]\n",
    "\n",
    "x = np.arange(len(objects))\n",
    "# print(v_all_a)\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, performanceV, width, label=\"Validation\")\n",
    "rects1 = ax.bar(x + width/2, performanceT, width, label=\"Training\")\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Training Points')\n",
    "ax.set_title('Accuracy for First Epoch')\n",
    "ax.set_xticks(x)\n",
    "labels = []\n",
    "totalV = sum(performanceV)\n",
    "totalT = sum(performanceT)\n",
    "\n",
    "for x in range(len(objects)):\n",
    "    labels.append(objects[x] + \"\\n\" + str(int((performanceV[x]/totalV)*100)) + \"%,\" + str(int((performanceT[x]/totalT)*100)) + \"%\")\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "plt.savefig('Acc_Zero_Epoch.png', dpi=300)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "x = np.arange(n_epochs)\n",
    "\n",
    "plt.plot(x, Train_loss, label=\"Training\")\n",
    "plt.plot(x, Val_loss, label=\"Validation\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Epochs vs Loss\")\n",
    "plt.savefig('Loss.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
